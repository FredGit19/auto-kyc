# ==============================================================================
# APPLICATION STREAMLIT - VERSION FINALE ET ROBUSTE
# ==============================================================================
# Corrections et am√©liorations cl√©s :
# - COH√âRENCE TOTALE : Utilise Albumentations pour le pr√©-traitement, comme dans 
#   le script d'entra√Ænement, afin d'√©liminer toute d√©synchronisation.
# - ROBUSTESSE : Gestion am√©lior√©e des erreurs de chargement et de d√©tection.
# - CLART√â : Code et commentaires am√©lior√©s pour une meilleure compr√©hension.
# ==============================================================================

import streamlit as st
import torch
import cv2
import numpy as np
import easyocr
from PIL import Image
import albumentations as A
from albumentations.pytorch import ToTensorV2
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

# --- CONFIGURATION ET CHARGEMENT DES MOD√àLES (S√©curis√© par cache) ---

MODEL_PATH = "models/frcnn_cni_best.pth" # Assurez-vous que le chemin est correct
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
CONFIDENCE_THRESHOLD = 0.7 # Seuil de confiance

def get_model_architecture(num_classes):
    """
    D√âFINITION IDENTIQUE √Ä L'ENTRA√éNEMENT :
    Construit l'architecture exacte du mod√®le pour pouvoir charger les poids.
    """
    model = fasterrcnn_resnet50_fpn(weights=None)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)
    return model

@st.cache_resource
def load_detection_model():
    """Charge le mod√®le de d√©tection Faster R-CNN entra√Æn√©."""
    try:
        # 1. Cr√©er l'architecture vide
        model = get_model_architecture(num_classes=2) # 1 classe (CNI) + 1 fond
        
        # 2. Charger le checkpoint (poids, etc.)
        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
        
        # 3. Charger les poids dans l'architecture
        model.load_state_dict(checkpoint['model_state_dict'])
        
        model.to(DEVICE)
        model.eval() # Mettre le mod√®le en mode √©valuation (tr√®s important !)
        print("Mod√®le de d√©tection charg√© avec succ√®s sur le device:", DEVICE)
        return model
    except FileNotFoundError:
        st.error(f"ERREUR CRITIQUE : Le fichier du mod√®le est introuvable au chemin '{MODEL_PATH}'.")
        st.error("V√©rifiez que le mod√®le 'frcnn_cni_best.pth' se trouve bien dans le dossier 'models/'.")
        return None
    except Exception as e:
        st.error(f"Une erreur est survenue lors du chargement du mod√®le de d√©tection : {e}")
        return None

@st.cache_resource
def load_ocr_model():
    """Charge le mod√®le EasyOCR pour la reconnaissance de texte."""
    try:
        reader = easyocr.Reader(['fr', 'en'], gpu=torch.cuda.is_available())
        print("Mod√®le OCR charg√© avec succ√®s.")
        return reader
    except Exception as e:
        st.error(f"Une erreur est survenue lors du chargement du mod√®le OCR : {e}")
        return None

# --- FONCTIONS DE TRAITEMENT AVEC LA CORRECTION D√âFINITIVE ---

def get_inference_transforms():
    """
    LA CORRECTION CL√â : Utilise le m√™me pipeline de validation que l'entra√Ænement.
    """
    return A.Compose([
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalisation ImageNet
        ToTensorV2(), # Convertit en Tensor PyTorch
    ])

def detect_cni(model, pil_image):
    """
    Effectue la d√©tection sur une image en utilisant le pipeline de transformation correct.
    """
    if model is None: return None, None
        
    # 1. Convertir l'image PIL en tableau NumPy, format attendu par Albumentations
    image_np = np.array(pil_image.convert('RGB'))
    
    # 2. Appliquer les transformations
    transforms = get_inference_transforms()
    transformed = transforms(image=image_np)
    image_tensor = transformed['image'].to(DEVICE)
    
    # 3. Le mod√®le attend un "batch" d'images, m√™me s'il n'y en a qu'une
    image_tensor = image_tensor.unsqueeze(0) 
    
    with torch.no_grad():
        prediction = model(image_tensor)
        
    # 4. Extraire et filtrer les r√©sultats
    boxes = prediction[0]['boxes'].cpu().numpy()
    scores = prediction[0]['scores'].cpu().numpy()
    
    best_box = None
    max_score = 0
    # On prend la bo√Æte avec le plus haut score au-dessus du seuil
    for box, score in zip(boxes, scores):
        if score > CONFIDENCE_THRESHOLD and score > max_score:
            max_score = score
            best_box = box

    # Dessiner le r√©sultat sur l'image originale (au format OpenCV)
    image_cv = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
    if best_box is not None:
        x1, y1, x2, y2 = map(int, best_box)
        cv2.rectangle(image_cv, (x1, y1), (x2, y2), (36, 255, 12), 2) # Couleur verte vive
        label = f"CNI: {max_score:.2f}"
        cv2.putText(image_cv, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (36, 255, 12), 2)
    
    return image_cv, best_box

def parse_ocr_results(ocr_results):
    """
    Tente de parser les r√©sultats bruts d'EasyOCR. C'est une heuristique.
    """
    # ... (le code de parsing reste le m√™me, il est √† adapter selon les r√©sultats)
    extracted_data = {"NOM": "Non trouv√©", "PRENOMS": "Non trouv√©", "DATE_NAISSANCE": "Non trouv√©"}
    full_text = " ".join([res[1] for res in ocr_results])
    
    for i, res in enumerate(ocr_results):
        text = res[1].upper()
        if "NOM" in text and i + 1 < len(ocr_results):
            extracted_data["NOM"] = ocr_results[i+1][1]
        if "PRENOMS" in text and i + 1 < len(ocr_results):
            extracted_data["PRENOMS"] = ocr_results[i+1][1]
        if "NEE LE" in text or "NE LE" in text and i + 1 < len(ocr_results):
            extracted_data["DATE_NAISSANCE"] = ocr_results[i+1][1]
            
    return extracted_data, full_text

# --- INTERFACE UTILISATEUR STREAMLIT ---

def main():
    st.set_page_config(page_title="Analyseur de CNI", layout="wide", initial_sidebar_state="auto")
    st.title("ü§ñ Analyseur de Carte Nationale d'Identit√©")
    st.markdown("---")

    st.sidebar.header("Instructions")
    st.sidebar.info(
        "1. Chargez une image claire et bien cadr√©e de la CNI.\n\n"
        "2. Cliquez sur le bouton 'Lancer l'analyse'.\n\n"
        "3. L'application va d'abord localiser la carte (bo√Æte verte), puis lire les informations qu'elle contient."
    )

    uploaded_file = st.file_uploader(
        "D√©posez une image ici ou cliquez pour parcourir vos fichiers", 
        type=["jpg", "jpeg", "png"]
    )

    if uploaded_file:
        col1, col2 = st.columns(2)
        pil_image = Image.open(uploaded_file)
        
        with col1:
            st.subheader("Image Originale")
            st.image(pil_image, use_column_width=True)

        if st.button("üöÄ Lancer l'analyse", use_container_width=True, type="primary"):
            detection_model = load_detection_model()
            ocr_model = load_ocr_model()

            if detection_model and ocr_model:
                with col2:
                    st.subheader("R√©sultats de l'Analyse")
                    with st.spinner("√âtape 1/2 : D√©tection de la carte en cours..."):
                        detected_image, box = detect_cni(detection_model, pil_image)
                    
                    if box is None:
                        st.warning("Aucune CNI n'a pu √™tre d√©tect√©e avec certitude. Essayez une image plus nette ou mieux cadr√©e.")
                        st.image(detected_image, channels="BGR", use_column_width=True, caption="Tentative de d√©tection.")
                    else:
                        st.success("‚úÖ CNI localis√©e !")
                        st.image(detected_image, channels="BGR", use_column_width=True, caption="CNI d√©tect√©e sur l'image.")
                        
                        with st.spinner("√âtape 2/2 : Lecture des informations (OCR)..."):
                            x1, y1, x2, y2 = map(int, box)
                            cni_crop = pil_image.crop((x1, y1, x2, y2))
                            ocr_results = ocr_model.readtext(np.array(cni_crop))
                            structured_data, raw_text = parse_ocr_results(ocr_results)
                        
                        st.success("‚úÖ Lecture termin√©e !")
                        st.write("#### Informations Extraites")
                        st.json(structured_data)
                        
                        with st.expander("Afficher le texte brut extrait"):
                            st.text_area("", raw_text, height=150)

if __name__ == "__main__":
    main()